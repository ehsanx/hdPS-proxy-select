---
# Supported options: 
#   sn-nature:       Style for submissions to Nature Portfolio journals
#   sn-basic:        Basic Springer Nature Reference Style/Chemistry Reference Style
#   sn-mathphys:     Math and Physical Sciences Reference Style
#   sn-aps:          American Physical Society (APS) Reference Style
#   sn-vancouver:    Vancouver Reference Style
#   sn-apa:          APA Reference Style 
#   sn-chicago:      Chicago-based Humanities Reference Style
#   default:         Default

classoptions: 
  - sn-vancouver      
  - Numbered      # Optional: Use numbered references instead of namedate references (only for sn-nature, sn-basic, sn-vancouver, sn-chicago, sn-mathphys or sn-nature)
  # - referee       # Optional: Use double line spacing 
  - lineno        # Optional: Add line numbers
  # - iicol         # Optional: Double column layout

title: Is There a Competitive Advantage to Using Multivariate Statistical or Machine Learning Methods Over the Bross Formula in the hdPS Framework for Bias and Variance Estimation?
titlerunning: High-Dimensional Propensity Score

authors: 
  - prefix: Dr.
    firstname: Mohammad Ehsanul 
    particle: 
    lastname: Karim
    suffix: 
    naturename: 
    degrees: MSc, PhD
    email: ehsan.karim@ubc.ca
    affiliation: [1,2]
    corresponding: TRUE
    equalcont: 
  - prefix: 
    firstname: Yang
    particle: 
    lastname: Lei
    suffix: 
    naturename: 
    degrees: 
    email: 
    affiliation: [3]
    corresponding: FALSE
    equalcont: 
affiliations:
  - number: 1
    corresponding: TRUE
    info:
      orgdiv: School of Population and Public Health
      orgname: University of British Columbia
    address:
        street: 2206 East Mall
        city: Vancouver
        postcode: V6T 1Z3
        state: BC
        country: Canada
  - number: 2
    corresponding: FALSE
    info:
      orgname: St. Paul’s Hospital
    address:
        street: 588 - 1081 Burrard Street
        city: Vancouver
        postcode: V6Z 1Y6
        state: BC
        country: Canada
  - number: 3
    corresponding: FALSE
    info:
      orgdiv: Department of Statistics
      orgname: University of British Columbia
    address:
        street: Room 3182 Earth Sciences Building, 2207 Main Mall
        city: Vancouver
        postcode: V6T 1Z4
        state: BC
        country: Canada      
        

keywords:
  - Machine learning
  - Propensity score 
  - Deep learning
  - Causal inference
  
pacs: 
  jel:
    - "C18"
  msc:
    - "92D30"
    - "62P10"

# Sample for structured abstract
abstract: |
  **Purpose**: We aims to evaluate various proxy selection methods within the context of high-dimensional propensity score (hdPS) analysis. The study focuses on assessing the performance of these methods, including alternative variable selection approaches, in selecting proxy variables for confounding adjustment compared to the traditional hdPS method that is rooted in Bross formula. The goal is to understand better the performance of these alternative methods in estimating treatment effects, and identify scenarios in which they may perform better.
  **Methods:** Using data from three cycles of the National Health and Nutrition Examination Survey (NHANES) spanning 2013-2018, we motivated the study by examining the association between obesity and diabetes. A plasmode simulation framework based on this data was employed to mimic real-world data structures. Simulations were conducted under three scenarios: Frequent Exposure and Outcome, Rare Exposure and Frequent Outcome, and Frequent Exposure and Rare Outcome. The performance of a variety of proxy selection methods—including tree-based methods, LASSO-based methods, and the Genetic Algorithm (GA)—was evaluated across these scenarios using standard simulation metrics.
  **Results:** XGBoost consistently demonstrated the lowest MSE and high coverage, making it a reliable method overall, although it did not always exhibit the lowest bias. In contrast, GA consistently showed the highest bias and MSE, with lower coverage and greater variability, indicating its unsuitability for accurate effect estimation. The kitchen sink model, Bross-based hdPS, and Hybrid hdPS methods performed moderately well, with low bias and moderate MSE, but varied in coverage across scenarios. Scenario-specific trends revealed that rare outcome scenarios yielded lower MSE and better precision, while rare exposure scenarios were associated with higher bias and MSE.
  **Conclusion:** Aside from GA, the performance of most other methods in your study appears to be comparable in each scenario, though with some variations depending on specific metrics. The findings underscore the importance of selecting appropriate methods for hdPS analysis based on the characteristics of the data, particularly the prevalence of exposure and outcome. While XGBoost excelled in overall accuracy and precision, the choice of method should be tailored to the specific epidemiological goal to optimize bias, coverage, and precision in estimating treatment effects.

bibliography: mergedbibliography.bib

header-includes: |
  \usepackage{natbib}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \usepackage{capt-of}
  \usepackage{booktabs}
  \usepackage{amssymb}
  \usepackage{threeparttable}
  \usepackage{float}
  %\floatplacement{figure}{H}
  %\floatplacement{table}{H}
  \usepackage{lipsum,caption}
  \geometry{
    a4paper,
    left=1in,
    right=1in,
    top=1in,
    bottom=1in,
    includeheadfoot
  }
  \usepackage{setspace}
  %\usepackage{lineno}
  %\linenumbers
  \usepackage{siunitx}
  \sisetup{
    mode = match,
    propagate-math-font = true,
    reset-math-version = false,
    reset-text-family = false,
    reset-text-series = false,
    reset-text-shape = false,
    text-family-to-math = true,
    text-series-to-math = true
  }
  \doublespacing
  % Disable explicit page breaks in LaTeX
  \let\clearpage\relax
output: rticles::springer_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # By default, hide code; set to TRUE to see code
  fig.pos = 'th', # Places figures at top or here
  out.width = '100%', dpi = 300, # Figure resolution and size
  fig.env="figure",
  out.extra = ""
) # Latex figure environment

options(knitr.table.format = "latex") # For kable tables to write LaTeX table directly
```

# Background

**High-dimensional Propensity Score (hdPS) Algorithm**: In epidemiology, proxy variables are commonly used as substitutes for confounders that are difficult or impossible to measure directly, such as socioeconomic status, lifestyle factors, or health behaviors [@vanderweele2019principles]. The high-dimensional propensity score (hdPS) is a pharmacoepidemiological method designed to reduce confounding bias in large healthcare databases [@schneeweiss2009high]. Unlike traditional propensity score models that rely on investigator-specified or manually selected covariates, hdPS automatically ranks a wide array of proxy variables from healthcare records—such as diagnosis codes, medications, and procedures—using the Bross formula [@wyss2018erratum; @bross1966spurious]. The Bross formula ranks these variables based on their marginal associations with both exposure and outcome. These selected proxy variables serve as surrogates for unmeasured or poorly measured confounders, helping reduce bias in treatment effect estimates. The hdPS algorithm further refines the selection by prioritizing variables based on their prevalence and potential for confounding, defined by their association with both exposure and outcome [@schneeweiss2009high].

**Multivariate Machine Learning Extensions**: Although the Bross formula performs well in certain contexts, it has limitations in capturing complex interactions, nonlinearities, and higher-order associations between variables, especially in high-dimensional settings where it does not account for the multivariate structure of other covariates [@karim2024high; @karim2018can]. To address these limitations, multivariate machine learning methods such as LASSO, Elastic Net, and Random Forests have been applied within the hdPS framework. These methods are better suited for high-dimensional data, where they can more effectively handle complex relationships and improve the selection of proxy variables, thus enhancing the precision of treatment effect estimates [@karim2018can; @schneeweiss2017variable; @franklin2015regularized]. Simulation studies and empirical research have shown that these machine learning-based methods, or hybrid approaches combining the Bross formula with machine learning, can reduce confounding more effectively and increase efficiency compared to the Bross formula alone in certain settings [@karim2018can; @schneeweiss2017variable; @franklin2015regularized].

**Assessing the Simulation Performance**: Other than bias, previous studies have primarily focused on Mean Squared Error (MSE) as a key metric for evaluating the performance of hdPS and its machine learning extensions [@franklin2015regularized; @pang2016effect; @wyss2018using; @karim2018can; @simon2023evaluating]. However, in high-dimensional settings with singly robust methods, such as hdPS and machine learning approaches like LASSO, MSE may not always be the most reliable measure. MSE combines both bias and variance into a single metric, which makes interpretation challenging when variance estimation is unstable—a common issue in these methods. In contrast, coverage, which measures the proportion of confidence intervals that capture the true treatment effect, provides a more direct and meaningful assessment of a model's reliability. In realistic observational studies, where model misspecification is often inevitable, coverage—along with related metrics such as bias-eliminated coverage and relative error in standard error (SE; which compares model-based SE with empirical SE)—can reveal whether confidence intervals or SEs are too narrow (underestimating uncertainty) or too wide (overestimating uncertainty). This insight is crucial in determining whether the model delivers valid estimates despite misspecification. Even a model with poor MSE but good coverage may still be valuable, as it produces realistic confidence intervals. By shifting the focus to coverage, rather than relying solely on MSE, we can achieve a more comprehensive understanding of method performance, especially in cases where unstable variance estimation might distort conclusions drawn from MSE alone.

**Aim**: This research aims to systematically evaluate and compare various proxy selection methods within the hdPS framework, using a diverse range of simulation performance metrics, including bias, MSE, and coverage. The study focuses on assessing how these alternative statistical and machine leartning methods perform in selecting proxy variables for confounding adjustment, compared to the traditional Bross formula. 

# Methods

## Data and Simulation {-}

**Motivating Example**: We revisited the association between obesity and the risk of diabetes using data from three cycles of the National Health and Nutrition Examination Survey (NHANES) covering the years 2013-2014, 2015-2016, and 2017-2018 [@karim2024high]. To identify relevant investigator-specified covariates, we constructed a causal diagram based on literature [@saydah2014trends;@liu2013association;@kabadi2012joint;@ostchega2012abdominal] and established causal inference principles [@greenland1999causal]. The covariates included in our analysis were carefully selected and categorized into demographic, behavioral, health history, access-related, and laboratory variables. While most of these variables were binary or categorical, the Laboratory variables were continuous.

**Plasmode simulation**: To rigorously assess the performance of the methods under consideration, we employed a plasmode simulation framework, which is particularly well-suited for reflecting real-world data structures and complexities [@franklin2014plasmode]. This approach was modeled after the analytic dataset derived from NHANES and involved resampling from the observed covariates and exposure information (i.e., obesity) without altering them. By mirroring key aspects of an actual epidemiological study, this simulation framework offers a substantial advantage over traditional Monte Carlo simulations, which often rely on hypothetical assumptions. 

**Simulation scenarios under consideration**: Our plasmode simulation was conducted over 500 iterations. For the base simulation scenario, we set the prevalence of exposure (obesity) and the event rate (diabetes) at 30%, with a true odds ratio (OR) parameter of 1, corresponding to a risk difference (RD) of 0. Each simulated dataset had a sample size of 3,000 participants. The description of other scenarios under consideration is provided in Table \ref{table:scenarios}.

\begin{table}[ht]
\centering
\caption{Overview of Plasmode Simulation Scenarios Reflecting Varying Exposure and Outcome Prevalences Based on National Health and Nutrition Examination Survey (NHANES) Data Cycles (2013-2018)}
\label{table:scenarios}
\begin{tabular}{lcccc}
  \toprule
  \textbf{Plasmode Simulation Scenario} & \textbf{Exposure} & \textbf{Outcome} & \textbf{True} & \textbf{Sample}\\
  \textbf{} & \textbf{Prevalence} & \textbf{Prevalence} & \textbf{Odds Ratio} & \textbf{Size}\\
  \midrule
  (i) Frequent Exposure and Outcome (Base) & 30\% & 30\% & 1 & 3,000 \\
  (ii) Rare Exposure and Frequent Outcome & 5\% & 30\% & 1 & 3,000 \\
  (iii) Frequent Exposure and Rare Outcome & 30\% & 5\% & 1 & 3,000 \\
  \bottomrule
\end{tabular}
\end{table}

**True Data Generating Mechanism Used in Plasmode Simulation**: The primary goal of this plasmode simulation study is to evaluate various variable selection methods under realistic conditions. To achieve this, we formulated the outcome data based on a specific model specification that incorporates both exposure and covariates, including investigator-specified and proxy variables. The model specification consists of three key components (See Appendices \S A and B for further details):

1. *Investigator-Specified Covariates*: We retained the original investigator-specified covariates, which were either binary or categorical, reflecting how real-world studies typically operate.

2. *Transformation of Laboratory Variables*: In real-world studies, it is common for analysts to lack precise knowledge of the true model specification. To simulate this uncertainty, we transformed the continuous laboratory variables using complex functions such as logarithmic, exponential, square root, polynomial transformations, and interactions. This reflects the challenges analysts face in correctly specifying models when dealing with continuous data.

3. *Inclusion of Proxy Variables*: Real-world studies often deal with unmeasured confounding, which researchers attempt to mitigate by adding proxy variables. However, when a large number of proxies are added, some may act as noise variables, contributing little or nothing to the analysis. To simulate this, we selected only those binary proxy covariates (referred to as recurrence covariates in hdPS terminology) that had a relative risk (RR) of less than 0.8 or greater than 1.2 concerning the outcome. Out of 142 proxy covariates, 94 met this criterion and were included in calculating a simple comorbidity burden measure. The remaining 48 covariates were excluded from this calculation and considered noise. This comorbidity burden measure (one variable) was then incorporated into our model specification for generating the outcome data.

**Performance Measures**: From this simulation, we derived several performance metrics to evaluate the effectiveness of the methods under consideration: (1) bias, (2) average model standard error (SE; the average of estimated SEs obtained from a model over repeated samples), (3) empirical SE (the standard deviation of estimated treatment effects across repeated samples), (4) mean squared error (MSE), (5) coverage probability of 95% confidence intervals, (6) bias-corrected coverage, and (7) Zip plot [@morris2019using; @white2023check].

## Estimators under consideration {-} 

The comparison between the data generation process and the analysis process reveals two key differences: (i) The data generation used transformed laboratory variables, whereas the analysis was conducted using only the original laboratory variables. (ii) The data generation employed a simple sum of selected proxy variables (sum of 94 proxy covariates), while the analysis included all proxy variables (142 binary proxies), with 48 of these acting as noise variables. These differences help us assess how the proxy variable selection methods handle model misspecification and the presence of noise variables.

1. **Kitchen sink model**: This is a base model for comparison, where no variable selection approaches were used. All investigator-selected features and all proxy variables were used to model [@karim2018can].  

2. **hdPS using Bross formula**: The Bross formula is a statistical method used to calculate the bias introduced by not adjusting for a covariate [@bross1966spurious]. In hdPS analysis, this formula was originally applied to each proxy variable to measure and rank the potential bias if the covariate were not adjusted for. In our analysis, the 100 proxies with the highest bias rankings are selected for further modeling [@schneeweiss2009high; @wyss2018erratum].

3. **Least Absolute Shrinkage and Selection Operator (LASSO)**: LASSO is a variable selection technique that limits the number of variables by adding a penalty term to the regression model. Cross-validation (CV) is used in LASSO to identify variables with non-zero coefficients in the best model by optimizing the penalty value [@franklin2015regularized; @schneeweiss2017variable; @karim2018can].

4. **Hybrid of hdPS and LASSO**: Instead of relying solely on LASSO for variable selection, a hybrid approach combines the Bross formula and LASSO. First, hdPS variables are selected using the hdPS algorithm (e.g., the top 100), and then LASSO is applied to further refine the selection [@karim2018can; @franklin2015regularized].

5. **Elastic Net**: Elastic Net is an extension of LASSO that includes an additional penalty term to handle multicollinearity by grouping correlated features and selecting the most representative ones [@karim2018can].

6. **Random Forest**: The Random Forest (RF) algorithm is an ensemble learning method that constructs multiple decision trees to perform classification [@breiman2001random]. It calculates the importance of each proxy variable based on the decrease in impurity or Gini importance, providing a ranking of the proxies. The top 100 variables from this ranking are manually selected for further modeling [@schneeweiss2017variable].

7. **XGBoost**: XGBoost is a gradient boosting algorithm used to optimize machine learning models [@chen2016xgboost]. It builds decision trees that make splits based on maximum impurity reduction, and it assigns an importance score to each proxy variable by calculating the mean decrease in impurity [@xiao2024interpretable].

8. **Stepwise**: Stepwise selection is a progressive feature selection method that can proceed in two directions—forward or backward—based on the maximum adjusted R-squared. We have implemented two versions: (a) Forward selection (FS) starts with an initial model (e.g., including all investigator-selected features) and adds proxies to the model one at a time. (b) Backward elimination (BE) starts with a full model (e.g., all investigator-selected features and all proxy variables) and removes features one at a time based on their contribution to the model.

9. **Genetic algorithm (GA)**: GA is an evolutionary algorithm inspired by the theory of natural selection [@holland1975adaptation]. It operates by evolving offspring from a population of the fittest individuals over several generations, evaluating and selecting the best combination of features or variables that maximize prediction accuracy.


# Results

The results for each method under the different scenarios are summarized below. See Figures \ref{fig:bias-comparison} and \ref{fig:Coverage-comparison} for an overview of the performance in terms of bias and coverage, respectively.

(i) **Frequent Exposure and Outcome (base) scenario**:

1. *Bias*: Bross-based hdPS exhibited the smallest bias (-0.0001), and the kitchen sink model (0.0002) was the second. GA shows the highest bias (0.0287), indicating a substantial deviation from the true effect. Among the other methods, Hybrid hdPS (0.0016), and Elastic Net (0.0036) demonstrated low bias. XGBoost (0.0074) had slightly higher bias than Random Forest (RF) (0.0034).

2. *Coverage*: The coverage for most methods was high, with Hybrid hdPS, Forward Selection, Backward Elimination, LASSO, and Elastic Net achieving values around 98%, indicating well-calibrated confidence intervals. However, GA had noticeably lower coverage (83.8%), indicating that its confidence intervals might be too narrow or biased, potentially missing the true effect. After applying bias elimination (as there were substantial bias associated with this method), GA's coverage improved to 96%.

3. *Mean Squared Error (MSE)*: XGBoost achieved the lowest MSE (0.0006). GA maintained the highest MSE (0.0016), reflecting its higher bias and variability. The kitchen sink model (0.0009), Bross-based hdPS (0.0008), Hybrid hdPS (0.0008), and Elastic Net (0.0009) all had relatively similar and moderate MSE values.

4. *Standard Error (SE)*: XGBoost exhibited the lowest Empirical SE (0.0229), indicating high precision in its estimates. The kitchen sink model had the highest Empirical SE (0.0305), suggesting greater variability. Other methods, including GA (0.0274), Hybrid hdPS (0.0278), and Bross-based hdPS (0.0287), showed moderate variability. LASSO (0.0299) and Elastic Net (0.0294) had slightly higher variability. The Model-based SE followed a similar pattern, with XGBoost (0.0268) showing the lowest variability and the kitchen sink model (0.0333) the highest, indicating less precision in its estimates. When comparing relative error in SE estimation, XGBoost performed the worst. See Appendix \S C for further details.

**(ii) Rare Exposure and Frequent Outcome**: 

1. *Bias*: The kitchen sink model showed a relatively low bias (0.0025), while GA continued to exhibit the highest bias (0.0408), indicating a significant deviation from the true effect. XGBoost had a bias of 0.0259, which, while still higher than other methods, but was lower than GA. Other methods, such as Bross-based hdPS (0.0035), Hybrid hdPS (0.0049), and Elastic Net (0.0053), demonstrated moderate bias. Random Forest (RF) (0.0127) and Forward Selection (0.0108) had slightly higher bias but remained within an acceptable range.

2. *Coverage*: Coverage levels remained high for most methods, with XGBoost achieving the highest coverage at 96.2%, indicating well-calibrated confidence intervals despite its higher bias. The GA method had lower coverage (92.2%), suggesting that its confidence intervals might be narrower, potentially missing the true effect. Other methods such as RF, Forward Selection, Backward Elimination, and Hybrid hdPS maintained coverage values around 94-95%, suggesting adequate interval calibration. Bias-eliminated coverage for GA improved to 94.2%, but it was still slightly lower than other methods (e.g., forward selection).

3. *Mean Squared Error (MSE)*: Forward selection demonstrated the lowest MSE (0.0030), and then Hybrid hdPS and XGBoost. The GA method had a higher MSE (0.0043), reflecting its substantial bias and variability. The kitchen sink model also had an MSE of 0.0043, similar to GA, while other methods like Bross-based hdPS (0.0035), RF (0.0039), and Elastic Net (0.0036) exhibited moderate MSE values, indicating reasonable accuracy.

4. *Standard Error (SE)*: The lowest Empirical SE was observed with XGBoost (0.0507) and GA (0.0510), reflecting high precision despite their higher bias. The kitchen sink model had the highest Empirical SE (0.0656), indicating greater variability. Hybrid hdPS (0.0564), Bross-based hdPS (0.0595), and RF (0.0609) showed moderate variability. Forward Selection (0.0537) and Backward Elimination (0.0576) had lower variability compared to the kitchen sink model. In terms of Model-based SE, XGBoost (0.0531) and GA (0.0533) continued to show low variability, while the kitchen sink model had the highest Model-based SE (0.0623), indicating less precision in its estimates. When comparing relative error in SE estimation, XGBoost and kitchen sink model performed the worst (in other direction). 


**(iii) Frequent Exposure and Rare Outcome**:

1. *Bias*: In this scenario, the kitchen sink model exhibited a moderate negative bias (-0.0093), similar to the Bross-based hdPS method (-0.0088). GA showed a significantly higher bias (0.0362), indicating a substantial deviation from the true effect. Among other methods, XGBoost demonstrated the lowest bias (-0.0061), while methods like Hybrid hdPS (-0.0082), Forward Selection (-0.0070), and Backward Elimination (-0.0070) had slightly higher but still moderate biases. Elastic Net and LASSO both had biases of -0.0079, reflecting slightly larger deviations compared to XGBoost but still within acceptable limits.

2. *Coverage*: Most methods achieved good coverage, with XGBoost, RF, and Forward Selection each achieving a coverage rate of 95.4%, indicating well-calibrated confidence intervals. The GA method, however, had slightly lower coverage (91.8%), indicating that its confidence intervals might be narrower, potentially excluding the true effect. Bross-based hdPS and the kitchen sink model had slightly lower coverage values of 93.8% and 93.4%, respectively. After accounting for bias, the bias-eliminated coverage for most methods, except GA, remained high, with values ranging from 98.4% to 99.0%, indicating that most methods effectively adjusted for bias in their coverage estimates. GA's bias-eliminated coverage was lower at 93.4%, reflecting its higher inherent bias.

3. *Mean Squared Error (MSE)*: XGBoost exhibited the lowest MSE (0.0003). GA had the highest MSE (0.0040), reflecting its substantial bias and variability. The kitchen sink model (0.0005), Bross-based hdPS (0.0005), and other methods like Hybrid hdPS (0.0004) and Elastic Net (0.0005) all had relatively similar MSE values, indicating moderate accuracy.

4. *Standard Error (SE)*: The lowest Empirical SE was observed with XGBoost (0.0152), reflecting high precision in its estimates. The GA method exhibited the highest Empirical SE (0.0523), indicating greater variability and less precision. Methods such as Hybrid hdPS (0.0184), Forward Selection (0.0187), and Elastic Net (0.0203) showed moderate variability, while Bross-based hdPS (0.0206) and the kitchen sink model (0.0212) had slightly higher variability. In terms of Model-based SE, XGBoost (0.0179) again showed the lowest variability, consistent with its low Empirical SE, indicating that it provided the most stable estimates. The kitchen sink model had a slightly higher Model-based SE (0.0219), indicating less precision in its estimates. When comparing relative error in SE estimation, XGBoost performed the worst. 

```{r, echo=FALSE, out.width="100%", fig.cap="Comparison of Bias Across Different Methods in hdPS Analysis\\label{fig:bias-comparison}", fig.align="center"}
knitr::include_graphics("figures/metric_comparison_Bias.png")
```

```{r, echo=FALSE, out.width="100%", fig.cap="Comparison of Coverage Probability Across Different Methods in hdPS Analysis\\label{fig:Coverage-comparison}", fig.align="center"}
knitr::include_graphics("figures/metric_comparison_Coverage.png")
```


# Real-world analysis

\textbf{Summary results}: The dataset comprises 7,585 individuals. Among these, the prevalence of the exposure is 48.8\%, while the prevalence of the outcome is 23.7\%. 

See Figure \ref{fig:comparison-plot-updated} for the results from analyzing the NHANES (2013-2018) dataset. The methods are arranged according to the number of selected proxy variables. Among all variable selection algorithms, Random Forest (RF) and XGBoost demonstrate the highest odds ratios (ORs), with values of 1.59 and 1.56, respectively. The ORs for the remaining methods cluster around 1.52. Additionally, with the exception of RF and the Bross formula hdPS, a general pattern emerges where methods selecting a larger number of proxy variables yield lower ORs. For risk difference (RD), RF and XGBoost also exhibit the highest value 0.08, while the remaining methods converge around 0.077. The trend observed in the OR results appears to persist in the RD analysis.

```{r, echo=FALSE, fig.align="center", fig.height=6, fig.width=8, fig.cap="Figure presenting a comparison of Risk Differences (RD) and Odds Ratios (OR) with 95\\% confidence intervals for different methods used to evaluate the association between obesity and diabetes risk. The analysis is based on data from the National Health and Nutrition Examination Survey (NHANES) for the years 2013-2018. Methods are arranged by the number of variables used in the models.\\label{fig:comparison-plot-updated}", message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)

# Combine RD and OR data into a single data frame
data_combined <- data.frame(
  Method = rep(c("KitchenSink", "Bross", "Hybrid", "LASSO", "ElasticNet", "RF", "XGBoost", "FS", "BE", "GA"), 2),
  # These are approximate values
  Value = round(c(0.075, 0.077, 0.079, 0.078, 0.075, 0.085, 0.082, 0.078, 0.078, 0.076, 
            1.502, 1.522, 1.543, 1.531, 1.504, 1.588, 1.564, 1.533, 1.533, 1.515),2),
  LowerCI = round(c(0.056, 0.058, 0.06, 0.059, 0.056, 0.066, 0.063, 0.059, 0.059, 0.057,
              1.395, 1.413, 1.432, 1.421, 1.396, 1.474, 1.452, 1.423, 1.423, 1.406),2),
  UpperCI = round(c(0.095, 0.096, 0.099, 0.098, 0.094, 0.104, 0.101, 0.097, 0.097, 0.096,
              1.618, 1.64, 1.663, 1.65, 1.62, 1.712, 1.685, 1.653, 1.653, 1.632),2),
  Measure = rep(c("RD", "OR"), each = 10),
  Variables = rep(c(142, 100, 49, 60, 69, 100, 48, 59, 59, 64), 2)
)

# Create a new Method label that includes the number of variables
data_combined <- data_combined %>%
  mutate(MethodLabel = paste(Method, " [", Variables, "]", sep = ""))

# Reorder the factor levels of MethodLabel based on the number of variables
data_combined$MethodLabel <- factor(data_combined$MethodLabel,
                                    levels = unique(data_combined$MethodLabel[order(data_combined$Variables)]))

# Create the plot with the reordered MethodLabel
ggplot(data_combined, aes(x = Value, y = MethodLabel)) +
  geom_point(size = 4) +
  geom_errorbarh(aes(xmin = LowerCI, xmax = UpperCI), height = 0.2) +
  geom_text(aes(label = Value, x = Value, y = MethodLabel), 
            vjust = -0.8, hjust = -0.5, size = 3) +
  facet_grid(. ~ Measure, scales = "free_x") +
  labs(x = "Value",
       y = "Method [Chosen number of proxies]") +
  theme_minimal() +
  theme(strip.text = element_text(size = 12, face = "bold"))
```

Table \ref{tab:method-comparison-updated} presents a pairwise comparison of the number of proxy features shared between different variable selection methods used in the analysis. Each cell in the table indicates the count of common proxy variables selected by the method in the corresponding row and column. The diagonal cells, where the row and column methods are the same, represent the total number of proxy variables selected exclusively by each method. The first column displays the number of proxy variables shared between each method and the kitchen sink (KS) model. Given that the KS model includes all proxy variables, the values in the first column are identical to those in the diagonal cells for each row, which also presents the total number of proxy variables selected by the method in the corresponding row.

\begin{table}[htbp]
\centering
\caption{Comparison of variable overlap of selected proxies across different methods used to evaluate the association between obesity and diabetes}
\label{tab:method-comparison-updated}
\begin{tabular}{lcccccccccc}
\toprule
 & \textbf{KS} & \textbf{Bross} & \textbf{Hybrid} & \textbf{LASSO} & \textbf{EN} & \textbf{RF} & \textbf{XGB} & \textbf{FS} & \textbf{BE} & \textbf{GA} \\
\midrule
\textbf{Kitchen sink (KS)} & 142 & & & & & & & & & \\
\textbf{Bross formula} & 100 & 100 & & & & & & & & \\
\textbf{Hybrid (Bross and LASSO)} & 49 & 49 & 49 & & & & & & & \\
\textbf{LASSO} & 60 & 47 & 47 & 60 & & & & & & \\
\textbf{Elastic Net (EN)} & 69 & 54 & 48 & 60 & 69 & & & & & \\
\textbf{Random Forest (RF)} & 100 & 71 & 38 & 46 & 52 & 100 & & & & \\  
\textbf{XGBoost (XGB)} & 48 & 38 & 24 & 28 & 30 & 37 & 48 & & & \\
\textbf{Forward selection (FS)} & 59 & 45 & 41 & 51 & 54 & 45 & 25 & 59 & & \\
\textbf{Backward elimination (BE)} & 59 & 45 & 41 & 51 & 54 & 45 & 25 & 59 & 59 & \\
\textbf{Genetic algorithm (GA)} & 64 & 44 & 28 & 36 & 40 & 49 & 25 & 35 & 35 & 64 \\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:proxy-common-pct-bross} presents a comparison of the count and percentage of common proxy variables selected by different methods in comparison with the Bross formula hdPS. The first column shows the total number of proxy variables selected by each method, while the second column lists the number of common features selected by both the respective method and the Bross formula. The third column reports the percentage of common features out of the total number of features selected by each method.

We observe that, aside from the Bross formula hdPS and Random Forest (RF) models, where the number of proxies was manually set to 100, and the kitchen sink (KS) model, which includes all proxies by design, the number of proxy variables selected by other models ranges between 49 and 69. As expected, the hybrid method combining Bross and LASSO hdPS selects exactly 49 proxy variables, all of which are selected by both methods, resulting in a common feature rate of 1.00. For other models, the common feature percentage is generally clustered around $74\%$, with XGBoost showing the highest common percentage at $79\%$, while the Genetic Algorithm (GA) displays the lowest common percentage at $69\%$.


\begin{table}[htbp]
\centering
\caption{Comparison of the count and percentage of proxy variables selected by each methods in common with that by the Bross formula hdPS}
\label{tab:proxy-common-pct-bross}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Total Count} & \textbf{Common Count} & \textbf{Rate in Common} \\ 
\midrule
\textbf{Kitchen sink} & 142 & - & - \\
\textbf{Bross formula} & 100 & - & - \\
\textbf{Hybrid (Bross and LASSO)} & 49 & 49 & 1.00 \\
\textbf{LASSO} & 60 & 47 & 0.78 \\
\textbf{Elastic Net} & 69 & 54 & 0.78 \\
\textbf{Random Forest (RF)} & 100 & 71 & 0.71 \\  
\textbf{XGBoost} & 48 & 38 & 0.79 \\
\textbf{Forward selection (FS)} & 59 & 45 & 0.76 \\
\textbf{Backward elimination (BE)} & 59 & 45 & 0.76 \\
\textbf{Genetic algorithm (GA)} & 64 & 44 & 0.69 \\
\bottomrule
\end{tabular}
\end{table}

**Computing time**: 

Figure \ref{fig:computing-time} presents the computing time for each method. All methods, aside from RF and GA, exhibit relatively fast computing times. RF and GA are generally much slower.


```{r, echo=FALSE, fig.align="center", fig.height=6, fig.width=8, fig.cap="Computing time for the real-world analysis for each algorithm under consideration. The analysis is based on data from the National Health and Nutrition Examination Survey (NHANES) for the years 2013-2018.\\label{fig:computing-time}", message=FALSE, warning=FALSE}
# Load necessary libraries
library(ggplot2)

# Create a data frame based on the table
computing_time_data <- data.frame(
  Method = c("Kitchen Sink", "Bross formula", "Hybrid (Bross and LASSO)", 
             "LASSO", "Elastic Net", "Random Forest (RF)", "XGBoost", 
             "Forward selection (FS)", "Backward elimination (BE)", "Genetic algorithm (GA)"),
  Computing_Time = c(2.942, 2.280, 2.517, 2.540, 2.814, 379.485, 1.444, 7.417, 12.794, 105.697)/60
)

# Plot the bar plot using ggplot2
ggplot(computing_time_data, aes(x = reorder(Method, Computing_Time), y = Computing_Time)) +
  geom_bar(stat = "identity", fill = "grey") + 
  coord_flip() +  # Flips the chart for better readability
  labs(title = "Computing Time for Each Algorithm", 
       x = "Method", 
       y = "Computing Time (minutes)") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Discussion

## Summary of the simulation findings

**Comparison of methods**:

Across the three scenarios, XGBoost consistently exhibited the lowest MSE and high coverage, making it one of the most reliable methods in terms of precision. However, XGBoost always displayed some level of bias, particularly when compared to methods such as the kitchen sink model, Bross-based hdPS, and Hybrid hdPS, which often showed lower bias in scenarios with frequent outcomes. In contrast, GA consistently exhibited the highest bias and MSE, along with lower coverage and greater variability, indicating that it was the least reliable method for accurate effect estimation. Methods such as Bross-based hdPS, Hybrid hdPS, and Elastic Net performed moderately well across all scenarios, striking a reasonable balance between bias, coverage, and MSE. However, they did not outperform XGBoost in terms of overall precision, particularly with respect to MSE, though those methods often had lower bias. The kitchen sink model, performed competitively with Bross-based hdPS in terms of bias and coverage, but did not perform well in terms of SEs and MSE.

**Comparison of scenarios**:

Scenarios with rare exposure tended to produce higher bias, particularly for methods like GA and XGBoost, while frequent outcomes generally led to lower bias for most methods. Overcoverage was common in the frequent exposure and outcome scenario, with several methods producing confidence intervals that were too wide (i.e., overestimating the uncertainty), while the other scenarios showed more balanced or slightly under-coverage. Frequent exposure scenarios also had higher relative error in standard error estimation, indicating greater difficulty in precisely estimating effects. In contrast, rare exposure scenarios had higher MSE, reflecting the challenge of estimating effects when exposure is uncommon. The rare outcome scenarios tended to exhibit the lowest MSE across most methods (with the exception of GA), further indicating that these scenarios provided a better balance between bias and precision for most methods.

## Contextualizing the literature

LASSO was previously shown to perform well in the contect of hdPS framework, particularly in terms of bias reduction and MSE [@karim2018can; @franklin2015regularized], with similar performance in Elastic Net [@karim2018can]. Our findings generally support these conclusions, with both methods demonstrating moderate bias and MSE across the scenarios. However, the performance of these methods was not consistent across all scenarios, with these methods showing higher bias in the rare exposure scenario in previous simulation studies [@karim2018can; @franklin2015regularized]. MSE of these methods are particularly high in rare exposure scenario. The hybrid method, which combines Bross-based hdPS and LASSO, also showed promising results, particularly in terms of MSE, indicating that it may be a suitable alternative to traditional hdPS methods. The Random Forest method also demonstrated similar performance in our study, with relatively low bias, as was also shown previously [@karim2018can]. However, none of these works focused on coverage, and did not consider other approaches we have considered here, such as GA, XGboost, forward selection or backward elimination. 

## Data analysis findings

The dataset used in the real-world analysis had frequent exposure (48.8% prevalence) and relatively moderate outcome rate (23.7%). RF and XGBoost demonstrated the highest ORs at 1.59 and 1.56, respectively, while the remaining methods clustered around an OR of 1.52. A general trend was observed where methods selecting fewer proxy variables yielded lower ORs, except for RF and the Bross formula hdPS. For RD, the estimates did not vary much. In terms of variable overlap, most methods shared around 74% of their proxy variables with the Bross formula hdPS, with XGBoost showing the highest common rate (79%) and Genetic Algorithm (GA) showing the lowest (69%). The computing time analysis revealed that, aside from RF and GA, most methods had relatively short computing times, with RF being the slowest by a significant margin.

## Strengths

Looking at the literature, almost all of the studies involving singly robust methods have focused on the performance of hdPS methods in terms of MSE [@franklin2015regularized; @pang2016effect; @wyss2018using; @karim2018can; @simon2023evaluating]. Our study extends this work by considering a wide variety of performance metrics that allows researchers to compare the results both in terms of bias and variance estimation. We also provide a comprehensive comparison of the statistical and machine learning methods across different scenarios, which has not been done before. For example, we found that XGBoost consistently demonstrated strong performance in terms of MSE and coverage, but it did not always have the lowest bias. We have also seen for the first time that standard statistical variable selection methods such as forward selection or backward elimination procedures do perform very similar to the LASSO or random forest approaches across scenarios, both in terms of bias and coverage.

## Future Direction

Double robust methods have been shown to be effective in achieving optimal statistical performance in hdPS analysis [@pang2016targeted; @pang2016effect; @benasseur2022comparison]. Additionally, single robust methods include ensemble learners, such as super learners, which have been effective in other contexts, particularly in improving bias, MSE, area under the curve, and covariate balance [@guertin2016head; @wyss2018using; @ju2019propensity]. However, despite the desirable theoretical properties of these methods, their complexity and computational burden, especially in high-dimensional settings, have limited their adoption by practitioners. In contrast, some of the singly robust machine learning methods we have discussed have been applied in addressing clinical questions [@hossain2023role; @basham2021post]. Future research could explore the potential of these double robust and super learner methods in hdPS analysis, particularly in the context of rare outcomes and exposures, where the performance of traditional methods may be suboptimal.

Furthermore, the impact of different hyperparameters on the performance of machine learning methods like XGBoost warrants further investigation to optimize their application in hdPS analysis. Lastly, conducting simulation studies to evaluate the performance of hdPS methods in terms of coverage in other epidemiological scenarios, such as time-varying exposures, could provide valuable insights into the generalizability of these findings [@neugebauer2015high].

## Conclusion

In conclusion, this analysis highlights the importance of carefully selecting appropriate methods for hdPS analysis based on the specific characteristics of the data, particularly the prevalence of exposure and outcome. While XGBoost consistently demonstrated strong performance in terms of MSE and coverage, it did not always have the lowest bias, suggesting that it may be most suitable when precision is prioritized over bias minimization. GA, despite its potential, showed significant limitations with consistently high bias and MSE, making it less reliable for accurate effect estimation. The kitchen sink model, Bross-based hdPS, and Hybrid hdPS methods provided a balanced approach, often delivering low bias and moderate MSE, but with variability in coverage depending on the scenario. Scenario-specific trends revealed that rare outcomes generally yielded lower MSE and better precision, while rare exposures were associated with higher bias and MSE, emphasizing the challenges of accurately estimating effects in such contexts. Ultimately, the findings underscore the need to tailor method selection to the epidemiological scenario at hand, ensuring that the chosen approach aligns with the specific goals and challenges of the analysis.

# List of abbreviations {-}

- hdPS: High-dimensional Propensity Score
- NHANES: National Health and Nutrition Examination Survey
- OR: Odds Ratio
- RD: Risk Difference
- SE: Standard Error
- MSE: Mean Squared Error
- KS: Kitchen Sink
- LASSO: Least Absolute Shrinkage and Selection Operator
- EN: Elastic Net; a regularized regression method that combines LASSO and Ridge regression
- RF: Random Forest
- XGBoost: Extreme Gradient Boosting
- FS: Forward Selection
- BE: Backward Elimination
- GA: Genetic Algorithm
- CV: Cross-Validation
- RR: Relative Risk

# Declarations {-}

## Ethics approval and consent to participate {-}

The analysis conducted on secondary and de-identified data is exempt from research ethics approval requirements. Ethics for this study was covered by item 7.10.3 in University of British Columbia's Policy #89: Research and Other Studies Involving Human Subjects 19 and Article 2.2 in of the Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans (TCPS2).

## Consent for publication {-}



## Availability of data and materials {-}



## Competing interests {-}

Over the past three years, MEK has received consulting fees from Biogen Inc. for consulting unrelated to this current work. MEK was previously supported by the Michael Smith Foundation for Health Research Scholar award. 

## Funding {-}

This work was supported by MEK's Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grants and Discovery Accelerator Supplements. 

## Authors' contributions {-}

MEK: Conceptualization, Writing – Original Draft, Review & Editing
YL: Formal Analysis, Review & Editing

## Acknowledgements {-}

Not applicable.

# References {-}

